{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "democratic-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "historical-therapy",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['problem_id', 'v0', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8',\n",
       "       ...\n",
       "       'v968', 'v969', 'v970', 'v971', 'v972', 'v973', 'v974', 'v975', 'v976',\n",
       "       'v977'],\n",
       "      dtype='object', length=979)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv('train_ml2_2021.csv')\n",
    "y_train = X_train['target'].copy()\n",
    "X_train.drop(['target'], inplace=True, axis=1)\n",
    "\n",
    "X_test = pd.read_csv('test0.csv')\n",
    "y_test = X_test['target'].copy()\n",
    "X_test.drop(['obs_id','target'], inplace=True, axis=1)\n",
    "\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "individual-forward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408.0"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.groupby('problem_id').count()['v0'].quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "naked-receipt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4124\n",
       "1    3175\n",
       "2     729\n",
       "3     255\n",
       "4      19\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "charged-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(np.array(X_train))\n",
    "y_train = torch.LongTensor(y_train)\n",
    "X_test = torch.FloatTensor(np.array(X_test))\n",
    "y_test = torch.LongTensor(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flying-sustainability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8302, 979])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "organized-coupon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4]), tensor([0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.unique(), y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "printable-concrete",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-aef7989d268a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 0 and 1, so we should be able to get away with scaling it to a normal distributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# for col in df_train.columns:\n",
    "#     data = df_train[col]\n",
    "#     title = f\"EDA of {col}\"\n",
    "    \n",
    "#     fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "#     ax.hist(data)\n",
    "#     plt.title(title)\n",
    "#     plt.show()\n",
    "\n",
    "# Oh wow lots of features. Looks like the majority are already scaled between\n",
    "# 0 and 1, so we should be able to get away with scaling it to a normal distributions\n",
    "\n",
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-nursing",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   sklearn.decomposition   import PCA\n",
    "from   sklearn.model_selection import *\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.pipeline        import Pipeline\n",
    "from   sklearn.preprocessing   import *\n",
    "from   sklearn.impute          import *\n",
    "from   sklearn.linear_model    import *\n",
    "from   sklearn.compose         import *\n",
    "from   sklearn.ensemble        import *\n",
    "from   sklearn.metrics         import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('train_ml2_2021.csv')\n",
    "y = X['target'].copy()\n",
    "X.drop(['target'], inplace=True, axis=1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y)\n",
    "\n",
    "X_test = pd.read_csv('test0.csv')\n",
    "y_test = X_test['target'].copy()\n",
    "X_test.drop(['obs_id','target'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(np.array(X_train).copy())\n",
    "y_train = torch.LongTensor(np.array(y_train).copy())\n",
    "X_valid =  torch.FloatTensor(np.array(X_valid).copy())\n",
    "y_valid =  torch.LongTensor(np.array(y_valid).copy())\n",
    "\n",
    "X_test =  torch.FloatTensor(np.array(X_test).copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-posting",
   "metadata": {},
   "source": [
    "# Working on a Real Model\n",
    "\n",
    "# Model #1: Basic 2 Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Neural_Network(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim):\n",
    "#         super(MultiLogisticRegression, self).__init__()\n",
    "#         ## CODE HERE\n",
    "#         self.hidden = nn.Linear(input_dim, 20)\n",
    "#         self.output = nn.Linear(20, 5)\n",
    "        \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         ## CODE HERE\n",
    "#         x = self.hidden(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         x = self.output(x)\n",
    "#         return x\n",
    "\n",
    "    \n",
    "def neural_network(hidden_dim, input_dim=979, num_classes=5):\n",
    "    net = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_dim, num_classes))\n",
    "    return net #.cuda()\n",
    "\n",
    "def train_epochs(model, x_train, y_train, x_valid, y_valid, epochs, lr=0.01, wd=1e-4):\n",
    "    ## get an optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    ## convert your training data to pytorch tensors\n",
    "    x = torch.FloatTensor(x_train)\n",
    "    y = torch.LongTensor(y_train)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        ## evaluate your training data to get y_hat\n",
    "        y_hat = model(x)\n",
    "        ## compute your loss\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        ## zero_grad\n",
    "        optimizer.zero_grad()\n",
    "        ## compute gradients\n",
    "        loss.backward()\n",
    "        ## call gradient descent\n",
    "        optimizer.step()\n",
    "        val_loss, val_acc, _, _ = valid_metrics(model, x_valid, y_valid)\n",
    "\n",
    "        if i%5 == 1:\n",
    "            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % \n",
    "                  (loss.item(), val_loss, val_acc))\n",
    "            \n",
    "def valid_metrics(model, x_valid, y_valid):\n",
    "    model.eval()\n",
    "    x = torch.FloatTensor(x_valid)\n",
    "    y = torch.LongTensor(y_valid)\n",
    "    out = model(x)\n",
    "#     print(out.shape)\n",
    "    loss = F.cross_entropy(out, y)\n",
    "#     print(test)\n",
    "#     print(\"Predicted Values: \")\n",
    "#     print(y_hat)\n",
    "#     print(\"Actual Values\")\n",
    "#     print(y)\n",
    "    _, y_pred = torch.max(out.data, 1) # \n",
    "    val_acc = y_pred.eq(y.data).sum() / y.size(0)\n",
    "    return loss.item(), val_acc.item(), out, y_pred\n",
    "\n",
    "def save_torch_to_csv(y_idx, sub_num):\n",
    "    output = pd.DataFrame(y_idx)\n",
    "    output.rename(columns={0:'target'}, inplace=True)\n",
    "    output.index.name = 'obs_id'\n",
    "    output.to_csv(f'submission{sub_num}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network(hidden_dim=100, input_dim = X_train.shape[1])\n",
    "train_epochs(model, X_train, y_train, X_valid, y_valid, epochs=100, lr=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-merchant",
   "metadata": {},
   "source": [
    "## Applying the model to the Validation Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, out, y_pred = valid_metrics(model, X_valid, y_valid)\n",
    "loss, acc, y_pred.unique()\n",
    "print(f\"test loss: {loss:.3f} test acc: {acc:.3f}    unique predicted values:{[int(i) for i in y_pred.unique()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-system",
   "metadata": {},
   "source": [
    "# Model #2: Two layer neural network with Batch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-apache",
   "metadata": {},
   "source": [
    "## Create the DataLoaders from .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "about-sandwich",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-142-25e7e5ebba75>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['label'] = y_train.copy()\n",
      "<ipython-input-142-25e7e5ebba75>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_valid['label'] = y_valid.copy()\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load in and split the data into train/test/split\n",
    "X = pd.read_csv('train_ml2_2021.csv')\n",
    "y = X['target'].copy()\n",
    "X.drop(['target'], inplace=True, axis=1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=X['problem_id'])\n",
    "X_train['label'] = y_train.copy()\n",
    "X_valid['label'] = y_valid.copy()\n",
    "\n",
    "# Save the split as a single csv so that MyDataset can read it in\n",
    "X_train.to_csv('train.csv')\n",
    "X_valid.to_csv('valid.csv')\n",
    "\n",
    "# Test set\n",
    "X_test = pd.read_csv('test0.csv')\n",
    "X_test.drop(['obs_id','target'], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# credit: https://www.reddit.com/r/pytorch/comments/f2c39s/how_to_create_a_data_loader_from_csv_file/\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root, n_features):\n",
    "        self.df = pd.read_csv(root, index_col='Unnamed: 0')\n",
    "        self.data = self.df.to_numpy()\n",
    "        self.x , self.y = (torch.from_numpy(self.data[:,:n_features]),\n",
    "                           torch.from_numpy(self.data[:,n_features:]))\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :], self.y[idx,:]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "train = MyDataset('train.csv', 979)\n",
    "train_dl = DataLoader(train, batch_size=200, shuffle=True)\n",
    "valid = MyDataset('valid.csv', 979)\n",
    "valid_dl = DataLoader(valid, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "distributed-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(hidden_dim, input_dim=979, num_classes=5):\n",
    "    net = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_dim, num_classes))\n",
    "    return net #.cuda()\n",
    "\n",
    "def train_model(model, train_dl, valid_dl, epochs, lr=0.01, wd=1e-4):  \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    for t in range(epochs):\n",
    "        # one epoch\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        for x, y in train_dl: \n",
    "            x = x.float()\n",
    "            y = y.long()\n",
    "            y_hat = model(x)\n",
    "            loss = F.cross_entropy(y_hat, y.squeeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        train_loss = total_loss/total\n",
    "        valid_loss, valid_acc, _ = valid_metric(model, valid_dl)\n",
    "        if t % 10 == 0: print(\"train loss %.3f valid loss %.3f valid acc %.3f\" % (train_loss, valid_loss, valid_acc))\n",
    "            \n",
    "def valid_metric(model, valid_dl):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for x, y in valid_dl: \n",
    "        x = x.float()\n",
    "        y = y.long()\n",
    "        out = model(x)\n",
    "        loss = F.cross_entropy(out, y.squeeze(1))\n",
    "        #print(loss.item())\n",
    "        total_loss += loss.item()*y.shape[0]\n",
    "        total += y.shape[0]\n",
    "        _, y_pred = torch.max(out.data, 1)\n",
    "        correct += y_pred.eq(y.data).sum()\n",
    "#         print(y_pred.eq(y.data).sum() / y.shape[0])\n",
    "#         print()\n",
    "    return total_loss/total, correct/total, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network(hidden_dim=50, input_dim = 979)\n",
    "train_model(model, train_dl, valid_dl, epochs=100, lr=.001, wd=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-stage",
   "metadata": {},
   "source": [
    "## Apply Model Two to Validation Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, y_pred = valid_metric(model, valid_dl)\n",
    "print(f\"test loss: {loss:.3f} test acc: {acc:.3f}    unique predicted values:{[int(i) for i in y_pred.unique()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-specification",
   "metadata": {},
   "source": [
    "# Model 3: Dropout Neural Network w/Batch Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "mighty-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_dropout(hidden_dim, p, input_dim=979, num_classes=5):\n",
    "    modules = []\n",
    "    modules.append(nn.Linear(input_dim, hidden_dim))\n",
    "    modules.append(nn.ReLU())\n",
    "    if p > 0:\n",
    "        modules.append(nn.Dropout(p))\n",
    "    modules.append(nn.Linear(hidden_dim, num_classes))\n",
    "    return nn.Sequential(*modules) #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-theorem",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = neural_network_dropout(hidden_dim=50, p=0.5)\n",
    "train_model(model, train_dl, valid_dl, epochs=100, lr=0.001, wd=0)\n",
    "loss, acc, y_pred = valid_metric(model, valid_dl)\n",
    "print(f\"test loss: {loss:.3f} test acc: {acc:.3f}    unique predicted values:{[int(i) for i in y_pred.unique()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-nursing",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vals = np.linspace(0.01, 1, 100)\n",
    "valid_acc = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for p in tqdm(p_vals):\n",
    "    model = neural_network_dropout(hidden_dim=50, p=0.1)\n",
    "    train_model(model, train_dl, valid_dl, epochs=100, lr=.001, wd=0)\n",
    "    loss, acc, y_pred = valid_metric(model, valid_dl)\n",
    "    valid_acc.append(acc)\n",
    "    valid_loss.append(loss)\n",
    "    loss, _, _ = valid_metric(model, train_dl)\n",
    "    train_loss.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "axes[0].plot(p_vals, train_loss, label='train')\n",
    "axes[0].plot(p_vals, valid_loss, label='valid')\n",
    "axes[0].set_xlabel('Dropout Rate')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[1].plot(p_vals, valid_acc, label='valid accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('Dropout Rate')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "plt.title('Optimal Dropout Rate', x=-0.1, y=1.05, fontsize=24)\n",
    "plt.savefig('graphs/Optimal_Dropout_Rate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vals[np.argmax(valid_acc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-northwest",
   "metadata": {},
   "source": [
    "- Will go with a dropout rate of 0.19, though the consistency in lower training loss shows overfitting is a major problem.\n",
    "\n",
    "\n",
    "Two Fixes:\n",
    "\n",
    "1. Number of training epochs\n",
    "2. Size of hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "valid_acc_ep = []\n",
    "train_loss_ep = []\n",
    "valid_loss_ep = []\n",
    "\n",
    "for e in tqdm(epochs):\n",
    "    model = neural_network_dropout(hidden_dim=50, p=0.19, seed=1)\n",
    "    train_model(model, train_dl, valid_dl, epochs=e, lr=.001, wd=0)\n",
    "    loss, acc, y_pred = valid_metric(model, valid_dl)\n",
    "    valid_acc_ep.append(acc)\n",
    "    valid_loss_ep.append(loss)\n",
    "    loss, _, _ = valid_metric(model, train_dl)\n",
    "    train_loss_ep.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "axes[0].plot(epochs, train_loss_ep, label='train')\n",
    "axes[0].plot(epochs, valid_loss_ep, label='valid')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('Number Training Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[1].plot(epochs, valid_acc_ep, label='valid')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('Number Training Epochs')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "plt.title('Optimal Epochs', x=-0.1, y=1.05, fontsize=24)\n",
    "plt.savefig('graphs/Optimal_Epochs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-psychology",
   "metadata": {},
   "source": [
    "- Appears that the optimal number of training epochs is 40. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200]\n",
    "valid_acc_hd = []\n",
    "train_loss_hd = []\n",
    "valid_loss_hd = []\n",
    "\n",
    "for hd in tqdm(hd_list):\n",
    "    model = neural_network_dropout(hidden_dim=hd, p=0.19)\n",
    "    train_model(model, train_dl, valid_dl, epochs=40, lr=.001, wd=0)\n",
    "    loss, acc, y_pred = valid_metric(model, valid_dl)\n",
    "    valid_acc_hd.append(acc)\n",
    "    valid_loss_hd.append(loss)\n",
    "    loss, _, _ = valid_metric(model, train_dl)\n",
    "    train_loss_hd.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "axes[0].plot(hd_list, train_loss_hd, label='train')\n",
    "axes[0].plot(hd_list, valid_loss_hd, label='valid')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('Hidden Layer Size')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[1].plot(hd_list, valid_acc_hd, label='valid')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('Hidden Layer Size')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "plt.title('Optimal Hidden Layer Size', x=-0.1, y=1.05, fontsize=24)\n",
    "plt.savefig('graphs/Optimal_Hidden_Layer_Size.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_list[np.argmax(valid_acc_hd)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-finland",
   "metadata": {},
   "source": [
    "- Looks like the optimal hidden layer size is 20. This is good news, as it closes the gap between the training and validation loss. Lowering the bias a little bit at a time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [.1, .05, .01, .005, .001, .0005, .0001, .00001]\n",
    "valid_acc_lr = []\n",
    "train_loss_lr = []\n",
    "valid_loss_lr = []\n",
    "\n",
    "for lr in tqdm(lr_list):\n",
    "    model = neural_network_dropout(hidden_dim=20, p=0.19)\n",
    "    train_model(model, train_dl, valid_dl, epochs=40, lr=lr, wd=0)\n",
    "    loss, acc, y_pred = valid_metric(model, valid_dl)\n",
    "    valid_acc_lr.append(acc)\n",
    "    valid_loss_lr.append(loss)\n",
    "    loss, _, _ = valid_metric(model, train_dl)\n",
    "    train_loss_lr.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "axes[0].plot(lr_list, train_loss_lr, label='train')\n",
    "axes[0].plot(lr_list, valid_loss_lr, label='valid')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('Learning Rate')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[1].plot(lr_list, valid_acc_lr, label='valid')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('Learning Rate')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "plt.title('Optimal Learning Rate', x=-0.1, y=1.05, fontsize=24)\n",
    "plt.savefig('graphs/Optimal_Learning_Rate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(train_loss_lr, valid_loss_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc_lr[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-bankruptcy",
   "metadata": {},
   "source": [
    "- training and accuracy loss seemed to be the most even for at a learning rate of $10^{-4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-quarterly",
   "metadata": {},
   "source": [
    "# Final Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network_dropout(hidden_dim=20, p=0.19)\n",
    "train_model(model, train_dl, valid_dl, epochs=40, lr=0.0001, wd=0)\n",
    "loss, acc, y_pred = valid_metric(model, valid_dl)\n",
    "print(f\"test loss: {loss:.3f} test acc: {acc:.3f}    unique predicted values:{[int(i) for i in y_pred.unique()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('test0.csv')\n",
    "X_test.drop(['obs_id','target'], inplace=True, axis=1)\n",
    "X_test = torch.FloatTensor(np.array(X_test))\n",
    "\n",
    "out = model(X_test)\n",
    "\n",
    "_, y_pred = torch.max(out.data, 1)\n",
    "\n",
    "save_torch_to_csv(y_pred, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-recipe",
   "metadata": {},
   "source": [
    "train_loss_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vals = np.linspace(0.01, 1, 100)\n",
    "valid_acc = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for p in tqdm(p_vals):\n",
    "    model = neural_network_dropout(hidden_dim=50, p=0.1)\n",
    "    train_model(model, train_dl, valid_dl, epochs=100, lr=.001, wd=0)\n",
    "    loss, acc, y_pred = valid_metric(model, valid_dl)\n",
    "    valid_acc.append(acc)\n",
    "    valid_loss.append(loss)\n",
    "    loss, _, _ = valid_metric(model, train_dl)\n",
    "    train_loss.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network_dropout(hidden_dim=50, p=0.1)\n",
    "train_model(model, train_dl, valid_dl, epochs=100, lr=.001, wd=0)\n",
    "loss, acc, y_pred = valid_metric(model, valid_dl)\n",
    "loss, acc, y_pred = valid_metric(model, valid_dl)\n",
    "print(f\"test loss: {loss:.3f} test acc: {acc:.3f}    unique predicted values:{[int(i) for i in y_pred.unique()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-pollution",
   "metadata": {},
   "source": [
    "## Save the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('test0.csv')\n",
    "X_test.drop(['obs_id','target'], inplace=True, axis=1)\n",
    "X_test = torch.FloatTensor(np.array(X_test))\n",
    "\n",
    "out = model(X_test)\n",
    "\n",
    "_, y_pred = torch.max(out.data, 1)\n",
    "\n",
    "save_torch_to_csv(y_pred, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-greensboro",
   "metadata": {},
   "source": [
    "# Last Push for an Optimal Model:\n",
    "\n",
    "One misunderstanding in the previous models is that the studies are related in some way. On further inspection/reflection of the data, this can be remedied by batching the data in a different way. Because we are graciously given the study that each observation came from in both the training and the test set, we will have the ability to train and test an ensemble of models based off the \"problem_id\" column.\n",
    "\n",
    "The neural networks below will still rely on the parameters given in the above final model, though we will now have an ensemble of 21 of them at our fingertips to apply to each test set.\n",
    "\n",
    "## Validating the Neural Network Models:\n",
    "\n",
    "It's often a good idea to see how well each individual neural network model performs, so let's take a look at the accuracy of each model while saving each successive trained model into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "criminal-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_ml2_2021.csv')\n",
    "x_col = batch1.columns[0:-1]\n",
    "y_col = batch1.columns[-1]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df[x_col], df[y_col], stratify=df[y_col])\n",
    "\n",
    "class BatchData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.to_numpy()\n",
    "        self.Y = y.to_numpy()\n",
    "        self.x , self.y = (torch.from_numpy(self.X),\n",
    "                           torch.from_numpy(self.Y))\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "def neural_network_dropout(hidden_dim, p, input_dim=979, num_classes=5):\n",
    "    modules = []\n",
    "    modules.append(nn.Linear(input_dim, hidden_dim))\n",
    "    modules.append(nn.ReLU())\n",
    "    if p > 0:\n",
    "        modules.append(nn.Dropout(p))\n",
    "    modules.append(nn.Linear(hidden_dim, 10))\n",
    "    modules.append(nn.ReLU())\n",
    "    if p > 0:\n",
    "        modules.append(nn.Dropout(p))\n",
    "    modules.append(nn.Linear(10, 25))\n",
    "    modules.append(nn.ReLU())\n",
    "    if p > 0:\n",
    "        modules.append(nn.Dropout(p))\n",
    "    modules.append(nn.Linear(25, 3))\n",
    "    modules.append(nn.ReLU())\n",
    "    if p > 0:\n",
    "        modules.append(nn.Dropout(p))\n",
    "    modules.append(nn.Linear(3, num_classes))\n",
    "    return nn.Sequential(*modules) #.cuda()\n",
    "\n",
    "# train = BatchData(X_train, y_train)\n",
    "# train_dl = DataLoader(train, batch_size = len(X_train)//10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "aware-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, valid_dl, epochs, lr=0.01, wd=1e-4):  \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    for t in range(epochs):\n",
    "        # one epoch\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        for x, y in train_dl: \n",
    "            x = x.float()\n",
    "            y = y.long()\n",
    "            y_hat = model(x)\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        train_loss = total_loss/total\n",
    "        valid_loss, valid_acc, _ = valid_metric(model, valid_dl)\n",
    "        #if t % 10 == 0: print(\"train loss %.3f valid loss %.3f valid acc %.3f\" % (train_loss, valid_loss, valid_acc))\n",
    "            \n",
    "def valid_metric(model, valid_dl):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    y_pred_tot = []\n",
    "    for x, y in valid_dl: \n",
    "        x = x.float()\n",
    "        y = y.long()\n",
    "        out = model(x)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        #print(loss.item())\n",
    "        total_loss += loss.item()*y.shape[0]\n",
    "        total += y.shape[0]\n",
    "        _, y_pred = torch.max(out.data, 1)\n",
    "        y_pred_tot.extend(y_pred)\n",
    "        correct += y_pred.eq(y.data).sum()\n",
    "#         print(y_pred.eq(y.data).sum() / y.shape[0])\n",
    "#         print()\n",
    "    return total_loss/total, correct/total, y_pred_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-recognition",
   "metadata": {},
   "source": [
    "## Ensemble of Models (One per Study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "organizational-jungle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:18<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(df[x_col], df[y_col], stratify=df[y_col])\n",
    "\n",
    "models = []\n",
    "accuracies = []\n",
    "sizes = []\n",
    "\n",
    "for i in tqdm(sorted(X_train['problem_id'].unique())):\n",
    "    # Focus on information from the problem id\n",
    "    X_batch = X_train[X_train['problem_id'] == i]\n",
    "    y_batch = y_train[X_train['problem_id'] == i]\n",
    "    X_val = X_valid[X_valid['problem_id'] == i]\n",
    "    y_val = y_valid[X_valid['problem_id'] == i]\n",
    "    \n",
    "    train = BatchData(X_batch, y_batch)\n",
    "    train_dl = DataLoader(train, batch_size = len(X_batch)//10, shuffle=True)\n",
    "    valid = BatchData(X_val, y_val)\n",
    "    valid_dl = DataLoader(valid, batch_size = len(X_val)//10, shuffle=False)\n",
    "    \n",
    "    model = neural_network_dropout(hidden_dim=40, p=0.1)\n",
    "    train_model(model, train_dl, valid_dl, epochs=100, lr=.001, wd=0)\n",
    "    \n",
    "    models.append(model)\n",
    "    _, _, y_pred = valid_metric(model, valid_dl)\n",
    "    sizes.append(len(X_batch)+len(X_val))\n",
    "    accuracies.append(accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-agriculture",
   "metadata": {},
   "source": [
    "## One model for all data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "secondary-jordan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6016377649325626"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = BatchData(X_train, y_train)\n",
    "train_dl = DataLoader(train, batch_size = len(X_train)//10, shuffle=True)\n",
    "valid = BatchData(X_valid, y_valid)\n",
    "valid_dl = DataLoader(valid, batch_size = len(X_val)//10, shuffle=False)\n",
    "model = neural_network_dropout(hidden_dim=40, p=0.1)\n",
    "train_model(model, train_dl, valid_dl, epochs=100, lr=0.001, wd=0)\n",
    "\n",
    "_, _, y_pred = valid_metric(model, valid_dl)\n",
    "\n",
    "acc = accuracy_score(y_pred, y_valid)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-hello",
   "metadata": {},
   "source": [
    "# Results:\n",
    "\n",
    "## Ensemble of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "urban-italian",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'study 0': [71.58, 369],\n",
       " 'study 1': [45.13, 408],\n",
       " 'study 2': [55.77, 408],\n",
       " 'study 3': [64.49, 408],\n",
       " 'study 4': [95.06, 347],\n",
       " 'study 5': [30.61, 369],\n",
       " 'study 6': [75.53, 393],\n",
       " 'study 7': [42.86, 388],\n",
       " 'study 8': [48.35, 369],\n",
       " 'study 9': [19.35, 352]}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studiespt1 = [f'study {i}' for i in range(10)]\n",
    "accpt1 = accuracies[:10]\n",
    "sizespt1 = sizes[:10]\n",
    "\n",
    "df_dict1 = {}\n",
    "\n",
    "for study, acc, size in zip(studiespt1, accpt1, sizespt1):\n",
    "    df_dict1[study] = [np.round(acc*100, 2), size]\n",
    "\n",
    "    \n",
    "studiespt2 = [f'study {i}' for i in range(11,21)]\n",
    "accpt2 = accuracies[11:21]\n",
    "sizespt2 = sizes[11:21]\n",
    "\n",
    "df_dict2 = {}\n",
    "\n",
    "for study, acc, size in zip(studiespt2, accpt2, sizespt2):\n",
    "    df_dict2[study] = [np.round(acc*100, 2), size]\n",
    "\n",
    "df_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "established-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study 0</th>\n",
       "      <th>study 1</th>\n",
       "      <th>study 2</th>\n",
       "      <th>study 3</th>\n",
       "      <th>study 4</th>\n",
       "      <th>study 5</th>\n",
       "      <th>study 6</th>\n",
       "      <th>study 7</th>\n",
       "      <th>study 8</th>\n",
       "      <th>study 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>71.58</td>\n",
       "      <td>45.13</td>\n",
       "      <td>55.77</td>\n",
       "      <td>64.49</td>\n",
       "      <td>95.06</td>\n",
       "      <td>30.61</td>\n",
       "      <td>75.53</td>\n",
       "      <td>42.86</td>\n",
       "      <td>48.35</td>\n",
       "      <td>19.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study size</th>\n",
       "      <td>369.00</td>\n",
       "      <td>408.00</td>\n",
       "      <td>408.00</td>\n",
       "      <td>408.00</td>\n",
       "      <td>347.00</td>\n",
       "      <td>369.00</td>\n",
       "      <td>393.00</td>\n",
       "      <td>388.00</td>\n",
       "      <td>369.00</td>\n",
       "      <td>352.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            study 0  study 1  study 2  study 3  study 4  study 5  study 6  \\\n",
       "accuracy      71.58    45.13    55.77    64.49    95.06    30.61    75.53   \n",
       "study size   369.00   408.00   408.00   408.00   347.00   369.00   393.00   \n",
       "\n",
       "            study 7  study 8  study 9  \n",
       "accuracy      42.86    48.35    19.35  \n",
       "study size   388.00   369.00   352.00  "
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(df_dict1)\n",
    "df1.index = ['accuracy', 'study size']\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "painful-detection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study 11</th>\n",
       "      <th>study 12</th>\n",
       "      <th>study 13</th>\n",
       "      <th>study 14</th>\n",
       "      <th>study 15</th>\n",
       "      <th>study 16</th>\n",
       "      <th>study 17</th>\n",
       "      <th>study 18</th>\n",
       "      <th>study 19</th>\n",
       "      <th>study 20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>44.3</td>\n",
       "      <td>80.68</td>\n",
       "      <td>60.17</td>\n",
       "      <td>56.88</td>\n",
       "      <td>62.5</td>\n",
       "      <td>28.15</td>\n",
       "      <td>41.77</td>\n",
       "      <td>54.03</td>\n",
       "      <td>43.0</td>\n",
       "      <td>51.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study size</th>\n",
       "      <td>310.0</td>\n",
       "      <td>320.00</td>\n",
       "      <td>547.00</td>\n",
       "      <td>408.00</td>\n",
       "      <td>408.0</td>\n",
       "      <td>547.00</td>\n",
       "      <td>352.00</td>\n",
       "      <td>469.00</td>\n",
       "      <td>469.0</td>\n",
       "      <td>408.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            study 11  study 12  study 13  study 14  study 15  study 16  \\\n",
       "accuracy        44.3     80.68     60.17     56.88      62.5     28.15   \n",
       "study size     310.0    320.00    547.00    408.00     408.0    547.00   \n",
       "\n",
       "            study 17  study 18  study 19  study 20  \n",
       "accuracy       41.77     54.03      43.0     51.16  \n",
       "study size    352.00    469.00     469.0    408.00  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(df_dict2)\n",
    "df2.index = ['accuracy', 'study size']\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-exemption",
   "metadata": {},
   "source": [
    "## Single Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "antique-toilet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data size</th>\n",
       "      <td>6226.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           All Data\n",
       "accuracy      0.602\n",
       "data size  6226.000"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_single = pd.DataFrame({'All Data': [np.round(acc,3), len(X_train)]})\n",
    "df_single.index = ['accuracy', 'data size']\n",
    "df_single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-geography",
   "metadata": {},
   "source": [
    "# Additional Practice Modeling (Scratch Work):\n",
    "\n",
    "- XGboost training on one batch at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stable-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "df = pd.read_csv('train_ml2_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "animal-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into 21 batches\n",
    "batches = []\n",
    "\n",
    "for id in df['problem_id'].unique():\n",
    "    batches.append(df[df['problem_id'] == id].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "joined-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = batches[0]\n",
    "batch2 = batches[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "turned-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = batch1.columns[0:-1]\n",
    "y_col = batch1.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "hired-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation set, making sure that each batch is with the other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "virgin-stamp",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(X_train['problem_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "resistant-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sequentially, building off each other:\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df[x_col], df[y_col], stratify=df[y_col])\n",
    "\n",
    "for i in sorted(X_train['problem_id'].unique()):\n",
    "    # Focus on information from the problem id\n",
    "    X_batch = X_train[X_train['problem_id'] == i]\n",
    "    y_batch = y_train[X_train['problem_id'] == i]\n",
    "    xg_vals = xgb.DMatrix(X_batch, label=y_batch)\n",
    "    params = {'objective': 'multi:softmax', \n",
    "              'num_class': len(y_train.unique()), \n",
    "              'max_depth': 9, \n",
    "              'verbosity': 0,\n",
    "              'gamma' : 2,\n",
    "              'eta' : 0.1,\n",
    "              'booster':'dart',\n",
    "              'sampling_method' : 'uniform'}\n",
    "    \n",
    "    if i == 0:\n",
    "        model = xgb.train(params, xg_vals, 100)\n",
    "        model.save_model('./xgb/model_0.model')\n",
    "    else:\n",
    "        model = xgb.train(params, xg_vals, 100, xgb_model=f'./xgb/model_{i-1}.model')\n",
    "        model.save_model(f'./xgb/model_{i}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "determined-links",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"Adaboost\" somewhat inspiration: weigh the different models via their accuracy score. \n",
    "models = []\n",
    "weights = []\n",
    "\n",
    "for i in sorted(X_train['problem_id'].unique()):\n",
    "    # Focus on information from the problem id\n",
    "    X_batch = X_train[X_train['problem_id'] == i]\n",
    "    y_batch = y_train[X_train['problem_id'] == i]\n",
    "    X_val = X_valid[X_valid['problem_id'] == i]\n",
    "    y_val = y_valid[X_valid['problem_id'] == i]\n",
    "    \n",
    "    xg_vals = xgb.DMatrix(X_batch, label=y_batch)\n",
    "    params = {'objective': 'multi:softmax', \n",
    "              'num_class': len(y_train.unique()), \n",
    "              'max_depth': 9, \n",
    "              'verbosity': 0,\n",
    "              'gamma' : 2,\n",
    "              'eta' : 0.1,\n",
    "              'booster':'dart',\n",
    "              'sampling_method' : 'uniform'}\n",
    "    \n",
    "    model = xgb.train(params, xg_vals, 100)\n",
    "    models.append(model)\n",
    "    y_pred = model.predict(xgb.DMatrix(X_val, label=y_val))\n",
    "    weights.append(accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "unusual-service",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_id</th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v968</th>\n",
       "      <th>v969</th>\n",
       "      <th>v970</th>\n",
       "      <th>v971</th>\n",
       "      <th>v972</th>\n",
       "      <th>v973</th>\n",
       "      <th>v974</th>\n",
       "      <th>v975</th>\n",
       "      <th>v976</th>\n",
       "      <th>v977</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 979 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   problem_id    v0    v1    v2    v3    v4    v5    v6    v7    v8  ...  \\\n",
       "0           2  0.60  0.05  0.50  0.79  0.06  0.72  0.51  0.34  0.41  ...   \n",
       "1           2  0.55  0.08  0.62  0.52  0.05  0.46  0.20  0.11  0.17  ...   \n",
       "2           2  0.35  0.85  0.42  0.39  0.04  0.68  0.54  0.55  0.10  ...   \n",
       "3           2  0.45  0.63  0.42  0.58  0.03  0.83  0.73  0.23  0.15  ...   \n",
       "4           2  0.47  0.11  0.45  0.78  0.43  0.57  0.66  0.39  0.68  ...   \n",
       "\n",
       "   v968  v969  v970  v971  v972  v973  v974  v975  v976  v977  \n",
       "0  0.72  0.50  0.34  0.72  0.12  0.66  0.75  0.52  0.74  0.35  \n",
       "1  0.22  0.68  0.68  0.78  0.17  0.45  0.50  0.59  0.57  0.74  \n",
       "2  0.26  0.77  0.40  0.72  0.79  0.29  0.47  0.49  0.75  0.63  \n",
       "3  0.24  0.76  0.42  0.30  0.06  0.40  0.56  0.42  0.81  0.54  \n",
       "4  0.37  0.69  0.76  0.34  0.10  0.61  0.56  0.40  0.40  0.45  \n",
       "\n",
       "[5 rows x 979 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test0.csv')[test_df.columns[1:-1]]\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "consecutive-decade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:39:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xg1 = xgb.DMatrix(X_train, label=y_train)\n",
    "params = {'objective': 'multi:softmax', 'num_class': len(batch1[y_col].unique())}\n",
    "model1 = xgb.train(params, xg1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fatty-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(xgb.DMatrix(X_valid, label=y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unnecessary-buying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6559139784946236"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "alert-ethnic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Train a booster with given parameters.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "params : dict\n",
       "    Booster params.\n",
       "dtrain : DMatrix\n",
       "    Data to be trained.\n",
       "num_boost_round: int\n",
       "    Number of boosting iterations.\n",
       "evals: list of pairs (DMatrix, string)\n",
       "    List of validation sets for which metrics will evaluated during training.\n",
       "    Validation metrics will help us track the performance of the model.\n",
       "obj : function\n",
       "    Customized objective function.\n",
       "feval : function\n",
       "    Customized evaluation function.\n",
       "maximize : bool\n",
       "    Whether to maximize feval.\n",
       "early_stopping_rounds: int\n",
       "    Activates early stopping. Validation metric needs to improve at least once in\n",
       "    every **early_stopping_rounds** round(s) to continue training.\n",
       "    Requires at least one item in **evals**.\n",
       "    The method returns the model from the last iteration (not the best one).\n",
       "    If there's more than one item in **evals**, the last entry will be used\n",
       "    for early stopping.\n",
       "    If there's more than one metric in the **eval_metric** parameter given in\n",
       "    **params**, the last metric will be used for early stopping.\n",
       "    If early stopping occurs, the model will have three additional fields:\n",
       "    ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.  Use\n",
       "    ``bst.best_ntree_limit`` to get the correct value if ``num_parallel_tree`` and/or\n",
       "    ``num_class`` appears in the parameters.  ``best_ntree_limit`` is the result of\n",
       "    ``num_parallel_tree * best_iteration``.\n",
       "evals_result: dict\n",
       "    This dictionary stores the evaluation results of all the items in watchlist.\n",
       "\n",
       "    Example: with a watchlist containing\n",
       "    ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
       "    a parameter containing ``('eval_metric': 'logloss')``,\n",
       "    the **evals_result** returns\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        {'train': {'logloss': ['0.48253', '0.35953']},\n",
       "         'eval': {'logloss': ['0.480385', '0.357756']}}\n",
       "\n",
       "verbose_eval : bool or int\n",
       "    Requires at least one item in **evals**.\n",
       "    If **verbose_eval** is True then the evaluation metric on the validation set is\n",
       "    printed at each boosting stage.\n",
       "    If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
       "    is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
       "    / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
       "    Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
       "    is printed every 4 boosting stages, instead of every boosting stage.\n",
       "xgb_model : file name of stored xgb model or 'Booster' instance\n",
       "    Xgb model to be loaded before training (allows training continuation).\n",
       "callbacks : list of callback functions\n",
       "    List of callback functions that are applied at end of each iteration.\n",
       "    It is possible to use predefined callbacks by using\n",
       "    :ref:`Callback API <callback_api>`.\n",
       "    Example:\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        [xgb.callback.reset_learning_rate(custom_rates)]\n",
       "\n",
       "Returns\n",
       "-------\n",
       "Booster : a trained booster model\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/3.8.1/lib/python3.8/site-packages/xgboost/training.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-raise",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
